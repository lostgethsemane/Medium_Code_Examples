{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOutkXuO7h4xPZEF3kdaOeH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lostgethsemane/Medium_Code_Examples/blob/master/Deep_Learning_Method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "W6DbrV4F0vg3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('dreaddit-train.csv')\n",
        "df_test = pd.read_csv('dreaddit-test.csv')"
      ],
      "metadata": {
        "id": "T5T7b2iovtC3"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.drop(df_train[df_train.text.str.strip().str.len() < 50].index, inplace=True)\n",
        "df_test.drop(df_test[df_test.text.str.strip().str.len() < 50].index, inplace=True)"
      ],
      "metadata": {
        "id": "7gDSCnp_wCDv"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train['text'].to_list()\n",
        "y_train = df_train['label'].to_list()\n",
        "X_test = df_test['text'].to_list()\n",
        "y_test = df_test['label'].to_list()"
      ],
      "metadata": {
        "id": "VW63PIxvwNO1"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "X_train_tokens = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt')\n",
        "X_test_tokens = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "gYiG5RGExVOp"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Defining batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Converting tokenized data to PyTorch tensors\n",
        "X_train_tensor = X_train_tokens['input_ids'].clone().detach()\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "X_test_tensor = X_test_tokens['input_ids'].clone().detach()\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "\n",
        "# Creating DataLoader for training data\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Creating DataLoader for testing data\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of batches in the Training DataLoader: {len(train_dataloader)}\")\n",
        "print(f\"Number of batches in the Training DataLoader: {len(test_dataloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sjLUZCUynz9",
        "outputId": "e5caab33-bc20-48c5-f401-c38312c4822b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in the Training DataLoader: 23\n",
            "Number of batches in the Training DataLoader: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "# freezing the parameters so these parameters don't get updated\n",
        "for p in model.base_model.parameters():\n",
        "    p.requires_grad = False\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "## checking which layers are frozen\n",
        "'''\n",
        "params = list(model.named_parameters())\n",
        "print(f\"The BERT model has {len(params)} different named parameters.\\n\")\n",
        "\n",
        "for name, param in params:\n",
        "    print(f\"{name} - {'Frozen' if not param.requires_grad else 'Not Frozen'} - {tuple(param.size())}\")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "-a_3qnEtli-v",
        "outputId": "f1a9d720-a5f8-4dfe-badd-e2e11fb8296c"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nparams = list(model.named_parameters())\\nprint(f\"The BERT model has {len(params)} different named parameters.\\n\")\\n\\nfor name, param in params:\\n    print(f\"{name} - {\\'Frozen\\' if not param.requires_grad else \\'Not Frozen\\'} - {tuple(param.size())}\")\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Starting time\n",
        "start_time = time.time()\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader))\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 5\n",
        "best_val_loss = float('inf')\n",
        "precision_per_epoch = []\n",
        "recall_per_epoch = []\n",
        "f1_per_epoch = []\n",
        "accuracy_per_epoch = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    i = 0\n",
        "    # Training loop\n",
        "    for batch in train_dataloader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        #if (i%5==0):\n",
        "        print(f'Batch {i+1} in epoch {epoch+1} finished training.')\n",
        "        i = i+1\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for batch in test_dataloader:\n",
        "        inputs, labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, labels=labels)\n",
        "        val_loss += outputs.loss.item()\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_dataloader)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    precision_per_epoch.append(precision)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    accuracy_per_epoch.append(precision)\n",
        "    recall = recall_score(all_labels, all_preds, zero_division=1)\n",
        "    recall_per_epoch.append(recall)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    f1_per_epoch.append(f1)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}')\n",
        "    print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy Score: {accuracy}')\n",
        "    # Check for early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "    else:\n",
        "        print(\"Validation loss is not improving. Stopping early.\")\n",
        "        break\n",
        "\n",
        "# Ending Training\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "IlcLd_kSggP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Time it took to train and evaluate the model: {end_time - start_time}\")"
      ],
      "metadata": {
        "id": "zxSBqeKASpdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving and downloading the model\n",
        "from google.colab import files\n",
        "torch.save(model.state_dict(), 'bert_initial_model_MS2.pth')\n",
        "#files.download('bert_initial_model_MS2.pth')"
      ],
      "metadata": {
        "id": "gT2Zkn9NS2hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the data for the analysis\n",
        "labels_last_epoch = all_labels\n",
        "predicitions_last_epoch = all_preds\n",
        "mismatched_indices = [i for i, (true, pred) in enumerate(zip(labels_last_epoch, predicitions_last_epoch)) if true != pred]"
      ],
      "metadata": {
        "id": "6uhAzaCXnmSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the mismatch for one example\n",
        " text = X_test[0]\n",
        " print(text)"
      ],
      "metadata": {
        "id": "foouSLlxPDfh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}