{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMhW6/cRbMTzym4XVzQ+t2Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lostgethsemane/Medium_Code_Examples/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6DbrV4F0vg3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
      ],
      "metadata": {
        "id": "jMULiPrf2yqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
      ],
      "metadata": {
        "id": "R3kdSLGu4KlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
      ],
      "metadata": {
        "id": "CZJB7lbyw1SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('dreaddit-train.csv')\n",
        "df_test = pd.read_csv('dreaddit-test.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "T5T7b2iovtC3",
        "outputId": "39ece92d-8037-4120-ada7-6ee8f51bf6e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2463e25ffae9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dreaddit-train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dreaddit-test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train['text'].to_list()\n",
        "y_train = df_train['label'].to_list()\n",
        "X_test = df_test['text'].to_list()\n",
        "y_test = df_test['label'].to_list()"
      ],
      "metadata": {
        "id": "VW63PIxvwNO1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.drop(df_train[df_train.text.str.strip().str.len() < 50].index, inplace=True)\n",
        "df_test.drop(df_test[df_test.text.str.strip().str.len() < 50].index, inplace=True)"
      ],
      "metadata": {
        "id": "7gDSCnp_wCDv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "X_train_tokens = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt')\n",
        "X_test_tokens = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "gYiG5RGExVOp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64"
      ],
      "metadata": {
        "id": "t5cwJHpTEmCG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert your tokenized data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_tokens['input_ids'])\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_tokens['input_ids'])\n",
        "y_test_tensor = torch.tensor(y_test)\n",
        "\n",
        "# Create DataLoader for training data\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create DataLoader for testing data\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "4sjLUZCUynz9",
        "outputId": "0d0f1040-8e73-4cb7-9111-314ff6932296"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-42712e780a99>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert your tokenized data to PyTorch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_train_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Assuming X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor, and tokenizer are defined\n",
        "# Model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader))\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 5\n",
        "best_val_loss = float('inf')\n",
        "precision_per_epoch = []\n",
        "recall_per_epoch = []\n",
        "f1_per_epoch = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    if (epoch==0): print('Model training started')\n",
        "    model.train()\n",
        "    if (epoch==0): print('Model training stopped')\n",
        "    if (epoch==0): print('Batches starting')\n",
        "    i = 0\n",
        "    for batch in train_dataloader:\n",
        "        print(\"batch number \", i)\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        i = i+1\n",
        "    if (epoch==0): print('Batches stopped')\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    if (epoch==0): print('Validation started')\n",
        "    for batch in test_dataloader:\n",
        "        inputs, labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, labels=labels)\n",
        "        val_loss += outputs.loss.item()\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    if (epoch==0): print('Validation stopped')\n",
        "    avg_val_loss = val_loss / len(test_dataloader)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    precision_per_epoch.extend(precision)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    recall_per_epoch.extend(recall)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    f1_per_epoch.extend(f1)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}')\n",
        "    print(f'Precision: {precision}, Recall: {recall}, F1 Score: {f1}')\n",
        "    # Check for early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "    else:\n",
        "        print(\"Validation loss is not improving. Stopping early.\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "SVL1025EyNkk",
        "outputId": "e7f34115-3f92-44de-83d3-43501e4e2d5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f2898712a7f6>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Optimizer and Scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_training_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ia2O5__Uz2y4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}